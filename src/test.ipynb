{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b03276d-4029-47e9-9c4f-19955b91c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from load import create_data_loader\n",
    "from train import train_deepfaker \n",
    "from models.baseline import CNN_DF, AE_DF\n",
    "import torch\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f840762-e837-4226-958e-c1d22d79d314",
   "metadata": {},
   "source": [
    "# Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d795735a-ec2f-4cbc-b3b3-d9cb4651d339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN_DF model...\n",
      "Using 2 GPUs for training.\n",
      "Epoch [1/1], Loss: 0.1946\n",
      "Total training time: 188.34 seconds\n",
      "Training AE_DF model...\n",
      "Using 2 GPUs for training.\n",
      "Epoch [1/1], Loss: 0.2152\n",
      "Total training time: 188.65 seconds\n",
      "Training completed for both models.\n"
     ]
    }
   ],
   "source": [
    "# Global Variables\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset_dir = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/dataset/raw\"\n",
    "checkpoint_dir_CNN_DF = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/src/checkpoints_main/cp_CNN_DF\"\n",
    "checkpoint_dir_AE_DF = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/src/checkpoints_main/cp_AE_DF\"\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "checkpoint_interval = 20\n",
    "\n",
    "# Ensure checkpoint directories exist\n",
    "os.makedirs(checkpoint_dir_CNN_DF, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir_AE_DF, exist_ok=True)\n",
    "\n",
    "# 1. Load Dataset\n",
    "train_loader = create_data_loader(dataset_dir)\n",
    "\n",
    "# 2. Models Instantiation\n",
    "cnn_model = CNN_DF()\n",
    "ae_model = AE_DF(input_dim=128 * 128)\n",
    "\n",
    "'''\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n",
    "    cnn_model = torch.nn.DataParallel(cnn_model)\n",
    "    ae_model = torch.nn.DataParallel(ae_model)\n",
    "    '''\n",
    "\n",
    "# 3. Train\n",
    "\n",
    "# 3.2 CNN\n",
    "print(\"Training CNN_DF model...\")\n",
    "train_deepfaker(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    checkpoint_dir=checkpoint_dir_CNN_DF,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    checkpoint_interval=checkpoint_interval,\n",
    ")\n",
    "# 3.1 Autoencoder\n",
    "print(\"Training AE_DF model...\")\n",
    "train_deepfaker(\n",
    "    model=ae_model,\n",
    "    train_loader=train_loader,\n",
    "    checkpoint_dir=checkpoint_dir_AE_DF,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    checkpoint_interval=checkpoint_interval,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed for both models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002b6f9-1538-438f-9624-8e78700d02c8",
   "metadata": {},
   "source": [
    "# Unaugmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dfae4af-f7bb-45d7-b8fd-ca77fb4beb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN_DF model...\n",
      "Using 2 GPUs for training.\n",
      "Epoch [1/1], Loss: 0.2010\n",
      "Total training time: 187.43 seconds\n",
      "Training AE_DF model...\n",
      "Using 2 GPUs for training.\n",
      "Epoch [1/1], Loss: 0.2159\n",
      "Total training time: 188.02 seconds\n",
      "Training completed for both models.\n"
     ]
    }
   ],
   "source": [
    "# Global Variables\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset_dir = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/dataset/raw\"\n",
    "checkpoint_dir_CNN_DF = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/src/checkpoints_main/cp_CNN_DF\"\n",
    "checkpoint_dir_AE_DF = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/src/checkpoints_main/cp_AE_DF\"\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "checkpoint_interval = 20\n",
    "\n",
    "# Ensure checkpoint directories exist\n",
    "os.makedirs(checkpoint_dir_CNN_DF, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir_AE_DF, exist_ok=True)\n",
    "\n",
    "# 1. Load Dataset\n",
    "train_loader = create_data_loader(dataset_dir)\n",
    "\n",
    "# 2. Models Instantiation\n",
    "cnn_model = CNN_DF()\n",
    "ae_model = AE_DF(input_dim=128 * 128)\n",
    "\n",
    "'''\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n",
    "    cnn_model = torch.nn.DataParallel(cnn_model)\n",
    "    ae_model = torch.nn.DataParallel(ae_model)\n",
    "    '''\n",
    "\n",
    "# 3. Train\n",
    "\n",
    "# 3.2 CNN\n",
    "print(\"Training CNN_DF model...\")\n",
    "train_deepfaker(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    checkpoint_dir=checkpoint_dir_CNN_DF,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    checkpoint_interval=checkpoint_interval,\n",
    ")\n",
    "# 3.1 Autoencoder\n",
    "print(\"Training AE_DF model...\")\n",
    "train_deepfaker(\n",
    "    model=ae_model,\n",
    "    train_loader=train_loader,\n",
    "    checkpoint_dir=checkpoint_dir_AE_DF,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    checkpoint_interval=checkpoint_interval,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed for both models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bafa4e5-3fb7-4ac7-b45c-654c9767b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility across Python, NumPy, and PyTorch.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set. Default is 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    Initialize each DataLoader worker with a seed.\n",
    "    \"\"\"\n",
    "    seed = torch.initial_seed() % (2**32)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "class MultiscaleImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for loading multiscale images stored in .tiff format.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory containing the raw images (./Multiscale_Image_Dataset/raw/).\n",
    "            transform (callable, optional): Optional transform to be applied on each image.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform        \n",
    "        self.image_paths = [\n",
    "            os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith(\".tiff\")\n",
    "        ]\n",
    "\n",
    "        if not self.image_paths:\n",
    "            raise FileNotFoundError(f\"No .tiff images found in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AugmentedMultiscaleImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Augmented dataset for loading and augmenting multiscale images.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None, target_size=1000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory containing the raw images (./Multiscale_Image_Dataset/raw/).\n",
    "            transform (callable, optional): Transform to apply to each image.\n",
    "            target_size (int): Total number of augmented images to create. Default is 10000.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size        \n",
    "        self.image_paths = [\n",
    "            os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith(\".tiff\")\n",
    "        ]\n",
    "\n",
    "        if not self.image_paths:\n",
    "            raise FileNotFoundError(f\"No .tiff images found in {root_dir}\")\n",
    "\n",
    "        self.base_size = len(self.image_paths) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        torch.manual_seed(idx)\n",
    "        random.seed(idx)\n",
    "        \n",
    "        original_idx = idx % self.base_size\n",
    "        img_path = self.image_paths[original_idx]\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        \n",
    "        augmentation_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomCrop((512, 512), pad_if_needed=True)            \n",
    "        ])\n",
    "\n",
    "        augmented_image = augmentation_transform(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented_image = self.transform(augmented_image)\n",
    "\n",
    "        return augmented_image\n",
    "\n",
    "\n",
    "def create_data_loader(root_dir, batch_size=32, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for training or evaluation.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Directory containing the raw dataset.\n",
    "        batch_size (int): Batch size for the DataLoader. Default is 32.\n",
    "        target_size (int): Total number of augmented images to create. Default is 10000.\n",
    "        mode (str): Mode for the DataLoader (\"train\" or \"eval\"). Default is \"train\".\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A DataLoader instance for the dataset.\n",
    "    \"\"\"\n",
    "    if mode not in [\"train\", \"eval\"]:\n",
    "        raise ValueError(f\"Mode must be 'train' or 'eval', got {mode}.\")\n",
    "\n",
    "    # Define transformations\n",
    "    \n",
    "    bright_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Lambda(lambda img: transforms.functional.adjust_brightness(img, 2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "\n",
    "    bright_dataset = AugmentedMultiscaleImageDataset(root_dir=dataset_dir, transform=bright_transform)\n",
    "    bright_loader = DataLoader(bright_dataset,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=(mode==\"train\"),\n",
    "                               num_workers=4,\n",
    "                               worker_init_fn=worker_init_fn)\n",
    "    return bright_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81f0f750-5e27-43c5-b2fb-38f071d7078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN_DF model...\n",
      "Using 2 GPUs for training.\n",
      "Epoch [1/1], Loss: 0.4266\n",
      "Total training time: 20.47 seconds\n",
      "Training AE_DF model...\n",
      "Using 2 GPUs for training.\n",
      "Epoch [1/1], Loss: 0.2379\n",
      "Total training time: 21.19 seconds\n",
      "Training completed for both models.\n"
     ]
    }
   ],
   "source": [
    "# Global Variables\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset_dir = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/dataset/raw\"\n",
    "checkpoint_dir_CNN_DF = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/src/checkpoints_main/cp_CNN_DF\"\n",
    "checkpoint_dir_AE_DF = \"/home/gridsan/yyao/Research_Projects/Microstructure_Enough/deep_faker/src/checkpoints_main/cp_AE_DF\"\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "checkpoint_interval = 20\n",
    "\n",
    "# Ensure checkpoint directories exist\n",
    "os.makedirs(checkpoint_dir_CNN_DF, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir_AE_DF, exist_ok=True)\n",
    "\n",
    "# 1. Load Dataset\n",
    "train_loader = create_data_loader(dataset_dir)\n",
    "\n",
    "# 2. Models Instantiation\n",
    "cnn_model = CNN_DF()\n",
    "ae_model = AE_DF(input_dim=128 * 128)\n",
    "\n",
    "'''\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training.\")\n",
    "    cnn_model = torch.nn.DataParallel(cnn_model)\n",
    "    ae_model = torch.nn.DataParallel(ae_model)\n",
    "    '''\n",
    "\n",
    "# 3. Train\n",
    "\n",
    "# 3.2 CNN\n",
    "print(\"Training CNN_DF model...\")\n",
    "train_deepfaker(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    checkpoint_dir=checkpoint_dir_CNN_DF,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    checkpoint_interval=checkpoint_interval,\n",
    ")\n",
    "# 3.1 Autoencoder\n",
    "print(\"Training AE_DF model...\")\n",
    "train_deepfaker(\n",
    "    model=ae_model,\n",
    "    train_loader=train_loader,\n",
    "    checkpoint_dir=checkpoint_dir_AE_DF,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    checkpoint_interval=checkpoint_interval,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training completed for both models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7f498-50b5-4478-9ba3-12d6dfba3d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
